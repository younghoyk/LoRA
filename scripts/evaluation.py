#!/usr/bin/env python3
"""
í†µí•© ì´ë¯¸ì§€ í‰ê°€ ì‹œìŠ¤í…œ (integrated_test_final.py)
ì½”ë“œì— ì§ì ‘ ì§€ì •ëœ ëŒ€í‘œ ì´ë¯¸ì§€ì™€ í´ë” ë‚´ì˜ ì—¬ëŸ¬ ì´ë¯¸ì§€ë“¤ì„ ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤.
ê° ì´ë¯¸ì§€ì— ëŒ€í•´ BLIP, CLIP, LPIPS í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
"""

import sys
import torch
from PIL import Image
import os
import time
from datetime import datetime
import json
import csv
import gc
# argparseëŠ” ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# import argparse 

# --- âœ… ì—¬ê¸°ì—ì„œ ëª¨ë“  ì„¤ì •ì„ ì§ì ‘ ìˆ˜ì •í•˜ì„¸ìš” ---
# LPIPS ë¹„êµì˜ ê¸°ì¤€ì´ ë  ëŒ€í‘œ ì´ë¯¸ì§€ ê²½ë¡œ
REPRESENTATIVE_IMAGE_PATH = "/Users/choimungi/Desktop/3-SV/AIM/evaluate/evaluatedPhoto/image.png"
# í‰ê°€í•  ì´ë¯¸ì§€ë“¤ì´ ë“¤ì–´ìˆëŠ” í´ë” ê²½ë¡œ
COMPARE_DIR_PATH = "/Users/choimungi/Desktop/3-SV/AIM/evaluate/compare_folder"
# ê²°ê³¼ íŒŒì¼ì„ ë‚´ë³´ë‚¼ì§€ ì—¬ë¶€ì™€ í˜•ì‹ ('json', 'csv', ë˜ëŠ” None)
EXPORT_FORMAT = 'csv' # 'json', 'csv', None ì¤‘ ì„ íƒ
# ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ (Noneìœ¼ë¡œ ë‘ë©´ COMPARE_DIR_PATHì— ì €ì¥ë©ë‹ˆë‹¤)
OUTPUT_DIR = None

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
ORIGINAL_PROMPT = "A wooden cabin with a smoking chimney stands among snow-covered trees and a snowy mountain in the back."
CLIP_PROMPT = "a detailed photo of a wooden log cabin with a tall stone chimney emitting smoke a thick pristine snow covers the roof snowy pine trees in the background a majestic mountain peak in the background under a clear blue sky a clear natural daylight a sharp focus a winter landscape view"
class IntegratedEvaluator:
    """í†µí•© ì´ë¯¸ì§€ í‰ê°€ í´ë˜ìŠ¤"""

    def __init__(self, representative_image_path, compare_dir_path, original_prompt, clip_prompt):
        self.representative_image_path = representative_image_path
        self.compare_dir = compare_dir_path
        self.original_prompt = original_prompt
        self.clip_prompt = clip_prompt
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.results = {}
        self.comparison_images = self._get_image_files()

        print("ğŸš€ í†µí•© ì´ë¯¸ì§€ í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ğŸ’» ì¥ì¹˜: {self.device}")
        print(f"ğŸ–¼ï¸ ëŒ€í‘œ ì´ë¯¸ì§€ (LPIPS ê¸°ì¤€): {os.path.basename(self.representative_image_path)}")
        print(f"ğŸ“‚ í‰ê°€ ëŒ€ìƒ í´ë”: {self.compare_dir}")
        print(f"ğŸ“Š í‰ê°€í•  ì´ë¯¸ì§€ ìˆ˜: {len(self.comparison_images)}ê°œ")
        print("=" * 80)

        if not self.comparison_images:
            print("âš ï¸ í‰ê°€í•  ì´ë¯¸ì§€ê°€ í´ë”ì— ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(0)

    def _get_image_files(self):
        """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        valid_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.webp']
        images = []
        if not os.path.isdir(self.compare_dir):
            print(f"âŒ ì˜¤ë¥˜: '{self.compare_dir}'ëŠ” ìœ íš¨í•œ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
            return []
        for filename in sorted(os.listdir(self.compare_dir)):
            if any(filename.lower().endswith(ext) for ext in valid_extensions):
                images.append(os.path.join(self.compare_dir, filename))
        return images

    def evaluate_blip(self, image_path):
        """BLIP-2 ëª¨ë¸ì„ ì‚¬ìš©í•œ ìº¡ì…”ë‹ + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í‰ê°€ (ë‹¨ì¼ ì´ë¯¸ì§€)"""
        start_time = time.time()
        try:
            from transformers import Blip2Processor, Blip2ForConditionalGeneration
            from sentence_transformers import SentenceTransformer, util

            print(" ğŸ§  BLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
            processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
            model = Blip2ForConditionalGeneration.from_pretrained(
                "Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16
            ).to(self.device)

            print(" âœï¸ ìº¡ì…˜ ìƒì„± ì¤‘...")
            image = Image.open(image_path).convert("RGB")
            inputs = processor(images=image, return_tensors="pt").to(self.device, torch.float16)

            with torch.no_grad():
                generated_ids = model.generate(**inputs, max_new_tokens=50)

            caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
            del model, processor, inputs, generated_ids
            gc.collect()
            torch.cuda.empty_cache()

            print(" ğŸ§® ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
            embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device=self.device)
            embedding_caption = embedding_model.encode(caption, convert_to_tensor=True)
            embedding_prompt = embedding_model.encode(self.original_prompt, convert_to_tensor=True)
            cosine_score = util.pytorch_cos_sim(embedding_caption, embedding_prompt).item()

            return {
                'score': cosine_score, 'caption': caption, 'time': time.time() - start_time,
                'status': 'success', 'description': 'BLIP-2 ìº¡ì…”ë‹ + ì½”ì‚¬ì¸ ìœ ì‚¬ë„'
            }
        except Exception as e:
            return {
                'score': None, 'caption': None, 'time': time.time() - start_time,
                'status': 'error', 'error': str(e), 'description': 'BLIP-2 ìº¡ì…”ë‹ + ì½”ì‚¬ì¸ ìœ ì‚¬ë„'
            }

    def evaluate_clip(self, image_path):
        """CLIP ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ í‰ê°€ (ë‹¨ì¼ ì´ë¯¸ì§€)"""
        start_time = time.time()
        try:
            import clip

            print(" ğŸ¯ CLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
            model, preprocess = clip.load("ViT-B/32", device=self.device)

            image = Image.open(image_path)
            image_input = preprocess(image).unsqueeze(0).to(self.device)
            tokens = clip.tokenize(self.clip_prompt, truncate=False).to(self.device)
            
            text_tokens = tokens[0, 1:torch.where(tokens[0] == 49407)[0][0]]
            chunks = [text_tokens[i:i + 75] for i in range(0, len(text_tokens), 75)]
            
            with torch.no_grad():
                image_features = model.encode_image(image_input)
                image_features /= image_features.norm(dim=-1, keepdim=True)

            text_features_list = []
            for chunk in chunks:
                chunk_with_sot_eot = torch.cat([
                    torch.tensor([49406], device=self.device), chunk, torch.tensor([49407], device=self.device)
                ])
                padded_chunk = torch.zeros(77, dtype=torch.long, device=self.device)
                padded_chunk[:len(chunk_with_sot_eot)] = chunk_with_sot_eot
                with torch.no_grad():
                    features = model.encode_text(padded_chunk.unsqueeze(0))
                    text_features_list.append(features)
            
            if text_features_list:
                text_features_avg = torch.stack(text_features_list).mean(dim=0)
                text_features_avg /= text_features_avg.norm(dim=-1, keepdim=True)
                similarity = (image_features @ text_features_avg.T).item()
            else:
                similarity = 0.0

            return {
                'score': similarity, 'time': time.time() - start_time, 'status': 'success',
                'description': 'CLIP ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„'
            }
        except Exception as e:
            return {
                'score': None, 'time': time.time() - start_time, 'status': 'error',
                'error': str(e), 'description': 'CLIP ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìœ ì‚¬ë„'
            }

    def evaluate_lpips(self, image_to_compare_path):
        """LPIPS ëª¨ë¸ì„ ì‚¬ìš©í•œ ì§€ê°ì  ìœ ì‚¬ë„ í‰ê°€ (ëŒ€í‘œ ì´ë¯¸ì§€ì™€ ë¹„êµ)"""
        start_time = time.time()
        try:
            # sys.path.append(os.path.join(os.path.dirname(__file__), "PerceptualSimilarity"))
            import lpips
            import torchvision.transforms as transforms

            print(" ğŸ” LPIPS ëª¨ë¸ ë¡œë”© ì¤‘...")
            loss_fn = lpips.LPIPS(net='alex', verbose=False).to(self.device)

            transform = transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])

            image1 = Image.open(self.representative_image_path).convert('RGB')
            image2 = Image.open(image_to_compare_path).convert('RGB')
            
            image1_tensor = transform(image1).unsqueeze(0).to(self.device)
            image2_tensor = transform(image2).unsqueeze(0).to(self.device)

            with torch.no_grad():
                lpips_distance = loss_fn.forward(image1_tensor, image2_tensor).item()

            converted_score = max(0, 1.0 - lpips_distance)
            similarity_percentage = converted_score * 100

            return {
                'score': lpips_distance, 'converted_score': converted_score,
                'similarity_percentage': similarity_percentage, 'time': time.time() - start_time,
                'status': 'success',
                'description': f'vs {os.path.basename(self.representative_image_path)}',
                'image1': os.path.basename(self.representative_image_path),
                'image2': os.path.basename(image_to_compare_path)
            }
        except Exception as e:
            return {
                'score': None, 'converted_score': None, 'similarity_percentage': None,
                'time': time.time() - start_time, 'status': 'error', 'error': str(e),
                'description': 'LPIPS ì§€ê°ì  ìœ ì‚¬ë„',
                'image1': os.path.basename(self.representative_image_path),
                'image2': os.path.basename(image_to_compare_path)
            }

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 90)
        print("ğŸ“Š í†µí•© í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("=" * 90)
        print(f"ğŸ–¼ï¸ ëŒ€í‘œ ì´ë¯¸ì§€: {os.path.basename(self.representative_image_path)}")
        print(f"ğŸ“ ì›ë³¸ í”„ë¡¬í”„íŠ¸: {self.original_prompt}")
        print(f"ğŸ“ CLIP í”„ë¡¬í”„íŠ¸: {self.clip_prompt[:60]}...")
        print(f"â° í‰ê°€ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        for img_name, results in self.results.items():
            print("\n" + "-" * 90)
            print(f"ğŸ“ í‰ê°€ ëŒ€ìƒ: {img_name}")
            print("-" * 90)
            print(f"{'ëª¨ë¸':<10} {'ìƒíƒœ':<10} {'ì ìˆ˜':<20} {'ì„¤ëª…':<30} {'ì†Œìš”ì‹œê°„':<10}")
            print("-" * 90)

            for model, result in results.items():
                status = "ì„±ê³µ" if result['status'] == 'success' else "ì‹¤íŒ¨"
                score_str = "N/A"
                if result.get('score') is not None:
                    if model == 'LPIPS':
                        score_str = f"{result['score']:.4f} ({result.get('similarity_percentage', 0):.1f}%)"
                    else:
                        score_str = f"{result['score']:.4f}"
                
                description = result['description'][:30]
                time_taken = f"{result['time']:.2f}s"
                print(f"{model:<10} {status:<10} {score_str:<20} {description:<30} {time_taken:<10}")

            if 'BLIP' in results and results['BLIP'].get('caption'):
                print(f"  ğŸ¤– BLIP ìº¡ì…˜: '{results['BLIP']['caption']}'")
        
        print("\n" + "=" * 90)
        print("ğŸ‰ ëª¨ë“  í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    def export_results(self, format='json', output_dir=None):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if output_dir is None:
            output_dir = self.compare_dir

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = f"evaluation_results_{timestamp}"
        
        export_data = {
            'metadata': {
                'representative_image_path': self.representative_image_path,
                'comparison_directory': self.compare_dir,
                'original_prompt': self.original_prompt,
                'clip_prompt': self.clip_prompt,
                'evaluation_time': datetime.now().isoformat(),
                'device': self.device
            },
            'results': self.results
        }

        if format and format.lower() == 'json':
            output_file = os.path.join(output_dir, f"{base_filename}.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ JSON ê²°ê³¼ ì €ì¥: {output_file}")

        elif format and format.lower() == 'csv':
            output_file = os.path.join(output_dir, f"{base_filename}.csv")
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Image', 'Model', 'Score', 'Status', 'Time_sec', 'Description', 'Additional_Info'])
                
                for img_name, results in self.results.items():
                    for model, result in results.items():
                        additional_info = ""
                        if model == 'BLIP' and result.get('caption'):
                            additional_info = f"Caption: {result['caption']}"
                        elif model == 'LPIPS' and result.get('converted_score') is not None:
                            additional_info = f"Similarity: {result['converted_score']:.4f} ({result.get('similarity_percentage', 0):.1f}%)"
                        
                        writer.writerow([
                            img_name, model, result.get('score', 'N/A'), result['status'],
                            f"{result['time']:.2f}", result['description'], additional_info
                        ])
            print(f"ğŸ“Š CSV ê²°ê³¼ ì €ì¥: {output_file}")
        
        return output_dir

    def run_evaluation(self, export_format=None, output_dir=None):
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        total_images = len(self.comparison_images)
        for i, image_path in enumerate(self.comparison_images):
            img_name = os.path.basename(image_path)
            print(f"\n{'='*20} ì´ë¯¸ì§€ í‰ê°€ ({i+1}/{total_images}): {img_name} {'='*20}")
            
            image_results = {}
            
            
            # 1. BLIP í‰ê°€
            print("\nğŸ“± BLIP í‰ê°€ ì‹œì‘...")
            blip_res = self.evaluate_blip(image_path)
            image_results['BLIP'] = blip_res
            print(f"âœ… BLIP ì™„ë£Œ: {blip_res.get('score', 'N/A'):.4f} (ì†Œìš”ì‹œê°„: {blip_res['time']:.2f}ì´ˆ)")
            gc.collect(); torch.cuda.empty_cache()
            

            # 2. CLIP í‰ê°€
            print("\nğŸ¯ CLIP í‰ê°€ ì‹œì‘...")
            clip_res = self.evaluate_clip(image_path)
            image_results['CLIP'] = clip_res
            print(f"âœ… CLIP ì™„ë£Œ: {clip_res.get('score', 'N/A'):.4f} (ì†Œìš”ì‹œê°„: {clip_res['time']:.2f}ì´ˆ)")
            gc.collect(); torch.cuda.empty_cache()
            
            
            # 3. LPIPS í‰ê°€
            print("\nğŸ” LPIPS í‰ê°€ ì‹œì‘...")
            lpips_res = self.evaluate_lpips(image_path)
            image_results['LPIPS'] = lpips_res
            print(f"âœ… LPIPS ì™„ë£Œ: {lpips_res.get('score', 'N/A'):.4f} (ì†Œìš”ì‹œê°„: {lpips_res['time']:.2f}ì´ˆ)")
            gc.collect(); torch.cuda.empty_cache()

            self.results[img_name] = image_results
        

        self.print_summary()
        
        if export_format:
            return self.export_results(format=export_format, output_dir=output_dir)
        
        return self.results

# --- â–¼â–¼â–¼ main í•¨ìˆ˜ ìˆ˜ì •ë¨ â–¼â–¼â–¼ ---
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # argparseë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì½”ë“œ ìƒë‹¨ì˜ ë³€ìˆ˜ë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
        evaluator = IntegratedEvaluator(
            representative_image_path=REPRESENTATIVE_IMAGE_PATH,
            compare_dir_path=COMPARE_DIR_PATH,
            original_prompt=ORIGINAL_PROMPT,
            clip_prompt=CLIP_PROMPT
        )
        # ì½”ë“œ ìƒë‹¨ì— ì„¤ì •ëœ EXPORT_FORMATê³¼ OUTPUT_DIR ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì‹¤í–‰í•©ë‹ˆë‹¤.
        evaluator.run_evaluation(export_format=EXPORT_FORMAT, output_dir=OUTPUT_DIR)

        if EXPORT_FORMAT:
            print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ê°€ {EXPORT_FORMAT.upper()} í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í‰ê°€ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()